{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CNN - ResNet\n",
    "This Program is using cats breed dataset with around 50 labels.\n",
    "There are two source that being used in this program. The good one from robots, and the worse is from keagle dataset.\n",
    "This prgram is using transferlearning with ResNet.\n",
    "## Outline\n",
    "- Import libraries\n",
    "- Import dataset\n",
    "- Split dataset\n",
    "- Data Augmentation\n",
    "- Model structuring\n",
    "- Model Training\n",
    "- Visualisasi Hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version: 2.8.0\n",
      "Using TensorFlow Dataset version: 4.5.2+nightly\n",
      "Using TensorFlow Hub version: 0.12.0\n",
      "Using Numpy version: 1.22.2\n",
      "GPU Device Found.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Using TensorFlow Dataset version: {tfds.__version__}\")\n",
    "print(f\"Using TensorFlow Hub version: {hub.__version__}\")\n",
    "print(f\"Using Numpy version: {np.__version__}\")\n",
    "print(f\"GPU Device Found.\" if tf.config.list_physical_devices('GPU') else \"GPU Device Not Found. Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Datasets\n",
    "importing dataset from local folder and also preparing stuff for data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path\n",
    "there are 3 option of dataset to use in this program. From Kaggle, Robots, and the merged one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_source = r\"../Dataset/\"\n",
    "dataset_all = r\"Merged Dataset All/\"\n",
    "dataset_kaggle = r\"Merged Dataset Kaggle/\"\n",
    "dataset_robot = r\"Robots Datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset review\n",
    "Chekking how many dataset classes and how many image per classes. dataset infomation is stored in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_review(dataset_type):\n",
    "    \"\"\"Checking what directory exist in datataset directory\n",
    "\n",
    "    Args:\n",
    "        dataset_type (string): root path for choosen dataset\n",
    "\n",
    "    Returns:\n",
    "        dictionary: store dictionary name as key and sum of image in it\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.join(path_source, dataset_type)\n",
    "    folders = {}\n",
    "    \n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folders[folder] = len(os.listdir(os.path.join(dataset_path, folder)))\n",
    "    \n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58 classes with 124386 images in total\n",
      "\n",
      "class Abyssinian have 428 images\n",
      "class American Bobtail have 919 images\n",
      "class American Curl have 139 images\n",
      "class American Shorthair have 4822 images\n",
      "class Applehead Siamese have 130 images\n",
      "class Balinese have 248 images\n",
      "class Bengal have 2544 images\n",
      "class Birman have 420 images\n",
      "class Bombay have 1919 images\n",
      "class British Shorthair have 753 images\n",
      "class Burmese have 335 images\n",
      "class Calico have 3345 images\n",
      "class catpedigree_merged have 6 images\n",
      "class Chartreux have 81 images\n",
      "class Cornish Rex have 159 images\n",
      "class Devon Rex have 112 images\n",
      "class Dilute Calico have 3121 images\n",
      "class Dilute Tortoiseshell have 3015 images\n",
      "class Domestic Long Hair have 4382 images\n",
      "class Domestic Medium Hair have 5323 images\n",
      "class Domestic Short Hair have 52105 images\n",
      "class Egyptian Mau have 500 images\n",
      "class Exotic Shorthair have 465 images\n",
      "class Extra-Toes Cat - Hemingway Polydactyl have 1154 images\n",
      "class Havana have 166 images\n",
      "class Himalayan have 1264 images\n",
      "class Japanese Bobtail have 125 images\n",
      "class Korat have 67 images\n",
      "class Maine Coon have 1578 images\n",
      "class Manx have 1991 images\n",
      "class Munchkin have 181 images\n",
      "class Nebelung have 144 images\n",
      "class Norwegian Forest Cat have 569 images\n",
      "class Ocicat have 117 images\n",
      "class Oriental Long Hair - oriental long hair have 37 images\n",
      "class Oriental Short Hair have 482 images\n",
      "class Oriental Tabby have 98 images\n",
      "class Persian have 4162 images\n",
      "class Pixiebob have 103 images\n",
      "class Ragamuffin have 116 images\n",
      "class Ragdoll have 2806 images\n",
      "class Russian Blue have 2048 images\n",
      "class Scottish Fold have 301 images\n",
      "class Selkirk Rex have 77 images\n",
      "class Siamese have 2296 images\n",
      "class Siberian have 156 images\n",
      "class Silver have 84 images\n",
      "class Snowshoe have 1597 images\n",
      "class Sphynx have 200 images\n",
      "class Sphynx - Hairless Cat have 209 images\n",
      "class Tabby have 2866 images\n",
      "class Tiger have 2209 images\n",
      "class Tonkinese have 250 images\n",
      "class Torbie have 3309 images\n",
      "class Tortoiseshell have 3788 images\n",
      "class Turkish Angora have 648 images\n",
      "class Turkish Van have 807 images\n",
      "class Tuxedo have 3110 images\n"
     ]
    }
   ],
   "source": [
    "datasetDict = dataset_review(dataset_all)\n",
    "\n",
    "temp = 0\n",
    "for key in datasetDict:\n",
    "    temp += datasetDict[key]\n",
    "\n",
    "print(f\"There are {len(datasetDict)} classes with {temp} images in total\", end=\"\\n\\n\")\n",
    "for key, value in datasetDict.items():\n",
    "    print(f\"class {key} have {value} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Preparation\n",
    "Creating training, testing, and/or validation directory for storing dataset split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(rooth_path, folders):\n",
    "    if os.path.isdir(os.path.join(rooth_path, \"training\")):\n",
    "        print(f\"this path {os.path.join(rooth_path, 'training')} already made\")\n",
    "    else:\n",
    "        os.makedirs(os.path.join(rooth_path, \"training\"))\n",
    "        for folder in folders.keys():\n",
    "            os.makedirs(os.path.join(rooth_path, f\"training/{folder}\"))\n",
    "    \n",
    "    if os.path.isdir(os.path.join(rooth_path, \"testing\")):\n",
    "        print(f\"this path {os.path.join(rooth_path, 'testing')} already made\")\n",
    "    else:\n",
    "        os.makedirs(os.path.join(rooth_path, \"testing\"))\n",
    "        for folder in folders.keys():\n",
    "            os.makedirs(os.path.join(rooth_path, f\"testing/{folder}\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this path ../Dataset/Used\\training already made\n",
      "this path ../Dataset/Used\\testing already made\n"
     ]
    }
   ],
   "source": [
    "create_dir(os.path.join(path_source, \"Used\"), datasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(sourcePath, trainPath, testPath, splitSize):\n",
    "    files = []\n",
    "    \n",
    "    for file_name in os.listdir(sourcePath):\n",
    "        if os.path.getsize(os.path.join(sourcePath, file_name)):\n",
    "            files.append(file_name)\n",
    "        else:\n",
    "            print(f\"{file_name} is ignored\")\n",
    "    \n",
    "    split_point = int(splitSize * len(files))\n",
    "    shuffled_file = random.sample(files, len(files))\n",
    "    \n",
    "    trainFile = shuffled_file[:split_point]\n",
    "    testFile = shuffled_file[split_point:]\n",
    "    \n",
    "    for file_train in trainFile:\n",
    "        shutil.copyfile(os.path.join(sourcePath, file_train),\n",
    "                        os.path.join(trainPath, re.sub(r\"([\\s/-]+)\", r\"_\", file_train).lower()))\n",
    "    for file_test in testFile:\n",
    "        shutil.copyfile(os.path.join(sourcePath, file_test),\n",
    "                        os.path.join(testPath, re.sub(r\"([\\s/-]+)\", r\"_\", file_test).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder Abyssinian image copy succes.\n",
      "folder American Bobtail image copy succes.\n",
      "folder American Curl image copy succes.\n",
      "folder American Shorthair image copy succes.\n",
      "folder Applehead Siamese image copy succes.\n",
      "folder Balinese image copy succes.\n",
      "folder Bengal image copy succes.\n",
      "folder Birman image copy succes.\n",
      "folder Bombay image copy succes.\n",
      "folder British Shorthair image copy succes.\n",
      "folder Burmese image copy succes.\n",
      "folder Calico image copy succes.\n",
      "dummy_data is ignored\n",
      "__pycache__ is ignored\n",
      "folder catpedigree_merged image copy succes.\n",
      "folder Chartreux image copy succes.\n",
      "folder Cornish Rex image copy succes.\n",
      "folder Devon Rex image copy succes.\n",
      "folder Dilute Calico image copy succes.\n",
      "folder Dilute Tortoiseshell image copy succes.\n",
      "folder Domestic Long Hair image copy succes.\n",
      "folder Domestic Medium Hair image copy succes.\n",
      "folder Domestic Short Hair copy is failed ===========\n",
      "folder Egyptian Mau copy is failed ===========\n",
      "folder Exotic Shorthair copy is failed ===========\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(os.path.join(path_source, dataset_all)):\n",
    "    train_dir = os.path.join(f\"{path_source}/Used\", f\"training/{folder}\")\n",
    "    test_dir = os.path.join(f\"{path_source}/Used\", f\"testing/{folder}\")\n",
    "    \n",
    "    try:\n",
    "        split_dataset(os.path.join(f\"{path_source}/{dataset_all}\", folder), train_dir, test_dir, 0.9)\n",
    "        print(f\"folder {folder} image copy succes.\")\n",
    "    except:\n",
    "        print(f\"folder {folder} copy is failed\", \"===========\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a698132df0d61d2c04481168cd60695d48e6b3ce384607f3b1f9e0e40157999"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
